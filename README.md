# MoCha Demo Implementation

This repository provides a **demo implementation** of [MoCha
Towards Movie-Grade Talking Character Synthesis](https://arxiv.org/pdf/2503.23307). built on top of HunyuanVideo.

We fine-tune HunyuanVideo on the Hallo3 dataset.
Due to differences in training data, model scale, and training strategy, this demo does not fully reproduce the performance of the original MoCha model, but it reflects the core design and and serves as a baseline for further research and study.

This implementation supports two generation modes:

- st2v: speech + text â†’ video
- sti2v: image + speech + text â†’ video

<br>

<a target="_blank" href="https://arxiv.org/pdf/2503.23307">
<img style="height:19pt" src="https://img.shields.io/badge/-Paper-black?style=flat&logo=arxiv"></a>
<a target="_blank" href="https://congwei1230.github.io/MoCha/">
<img style="height:19pt" src="https://img.shields.io/badge/-ðŸŒ%20Website-black?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoChaBench">
<br>
<a target="_blank" href="https://github.com/congwei1230/MoCha-Demo">
<img style="height:19pt" src="https://img.shields.io/badge/-Code-red?style=flat&logo=github"></a>
<a target="_blank" href="https://github.com/congwei1230/MoChaBench/tree/main/benchmark">
<img style="height:19pt" src="https://img.shields.io/badge/-MoChaBench-red?style=flat&logo=github"></a>
<a target="_blank" href="https://github.com/congwei1230/MoChaBench/tree/main/mocha-generation">
<img style="height:19pt" src="https://img.shields.io/badge/-Our Results on MoChaBench-red?style=flat&logo=github"></a>
<br>
<a target="_blank" href="https://huggingface.co/CongWei1230/MoCha-Demo">
<img style="height:19pt" src="https://img.shields.io/badge/-ðŸ¤—%20Model-yellow?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoChaBench">
<img style="height:19pt" src="https://img.shields.io/badge/-ðŸ¤—%20 MoChaBench HF Version-yellow?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoCha-Generation-on-MoChaBench">
<img style="height:19pt" src="https://img.shields.io/badge/-ðŸ¤—%20Visualizing Our Results on MoChaBench-yellow?style=flat"></a>
<br> 


Many thanks to the community for sharing â€” 
[An emotional narrative](https://x.com/CongWei1230/status/1907879690959732878), created with light manual editing on clips generated by **MoCha**, has surpassed **1 million views** on X. <br>
<a target="_blank" href="https://x.com/AngryTomtweets/status/1907036631057752164">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet1-blue?style=flat&logo=twitter"></a>
<a target="_blank" href="https://x.com/_akhaliq/status/1906935462075236621">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet2-blue?style=flat&logo=twitter"></a>
<a target="_blank" href="https://x.com/minchoi/status/1907265748721889383">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet3-blue?style=flat&logo=twitter"></a>
<br>

<div align="center">
  <img src="assets/narrative.gif" alt="MoCha" width="70%"/>
</div>

<br>


## ðŸ””News

- **ðŸ”¥[2025-12-27]: Released a demo implementation built on HunyuanVideo â€” [**ðŸ¤— Checkpoints**](https://huggingface.co/CongWei1230/MoCha-Demo) and [**Code**](https://github.com/congwei1230/MoCha-Demo)**


## How to use

### 1. Create Conda Environment

```
conda env create -f environment.yml
conda activate mocha
```

This environment is tested with:
- Python 3.11
- PyTorch 2.4.1 + CUDA 12.1
- diffusers 0.36.0
- transformers 4.49.0


### 2. Download Checkpoint

Download the [MoCha transformer checkpoint](https://huggingface.co/CongWei1230/MoCha-Demo) to a local path:

```
python download_ckpt.py
```

After downloading, record the local path to the checkpoint file (e.g. model.ckpt).


### 3. Inference


Speech + Text â†’ Video (st2v)

```
python inference.py \
  --task st2v \
  --audio_path demos/man_1.mp3 \
  --output_path demos/output.mp4 \
  --transformer_ckpt_path /path/to/your/model.ckpt
```

Speech + Image + Text â†’ Video (sti2v)

```
python inference.py \
  --task sti2v \
  --audio_path demos/man_1.mp3 \
  --i2v_img_path demos/man_1.png \
  --output_path demos/output.mp4 \
  --transformer_ckpt_path /path/to/your/model.ckpt
```


## Citation

ðŸŒŸ If you find this work useful, please give us a free cite:

```bibtex
@article{wei2025mocha,
  title={Mocha: Towards movie-grade talking character synthesis},
  author={Wei, Cong and Sun, Bo and Ma, Haoyu and Hou, Ji and Juefei-Xu, Felix and He, Zecheng and Dai, Xiaoliang and Zhang, Luxin and Li, Kunpeng and Hou, Tingbo and others},
  journal={arXiv preprint arXiv:2503.23307},
  year={2025}
}
```